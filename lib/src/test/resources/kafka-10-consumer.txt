/**
 * Generated by apidoc - http://www.apidoc.me
 * Service version: 1.0.0
 */

import java.util.Properties

import scala.language.postfixOps
import scala.annotation.tailrec
import scala.collection.JavaConversions._
import scala.util.matching.Regex
import scala.util.{ Try, Success }

import com.typesafe.config.Config

import play.api.libs.json.Json

import org.apache.kafka.clients.consumer.KafkaConsumer
import org.apache.kafka.clients.consumer.ConsumerRecord
import org.apache.kafka.clients.consumer.ConsumerRecords
import org.apache.kafka.clients.consumer.ConsumerRebalanceListener
import org.apache.kafka.common.serialization.StringDeserializer
import org.apache.kafka.common.TopicPartition

import movio.api.kafka_0_10.Consumer

package test.apidoc.apidoctest.v0.kafka {
  import test.apidoc.apidoctest.v0.models._
  import test.apidoc.apidoctest.v0.models.json._

  object KafkaMemberTopic {
    /**
      The version of the api - apidoc generator enforces this value.
      For use when creating a topic name.
      Example: "v2"
      */
    val apiVersion = "v0"

    /**
      The name of the kafka topic to publish and consume records from.
      This is a scala statedment/code that that gets executed
      Example: `s"mc-servicename-${apiVersion}-${instance}-${tenant}"`

      @param instance an instance of the topic, eg uat, prod. It's read from the config.
      @param tenant is the customer id, eg vc_regalus
      */
    def topic(instance: String)(tenant: String) = s"mc.data.member-internal.$instance.$tenant"

    /**
      The regex for the kafka consumer to match topics.

      @param instance an instance of the topic, eg uat, prod. It's read from the config.
      @param tenants the tenants of the topics from which the consumer consumes. If it's empty,
             all tenants are matched.
      */
    def topicRegex(inst: String, tenants: Seq[String]) = {
      val instance = Regex.quote(inst)
      val tenantsPattern = if (tenants.isEmpty) ".*"
                           else tenants.map(Regex.quote(_)).mkString("|")

      s"mc.data.member-internal.$instance.($tenantsPattern)"
    }
  }

  object KafkaMemberConsumer {
    val base = "test.apidoc.apidoctest.kafka.consumer"
    val BootstrapServers = s"$base.bootstrap.servers"
    val TopicInstanceKey = s"$base.topic.instance"
    val TenantsKey = s"$base.tenants"
    val PollTimeoutKey = s"$base.poll.timeout" // ms
  }

  class KafkaMemberConsumer (
    config: Config,
    consumerGroupId: String,
    tenants: Option[Seq[String]] = None
  ) extends Consumer[KafkaMember] {
    import KafkaMemberConsumer._

    val pollMillis = config.getLong(PollTimeoutKey)

    lazy val topicRegex: Regex =
      KafkaMemberTopic.topicRegex(
        config.getString(TopicInstanceKey),
        tenants.getOrElse(config.getStringList(TenantsKey))
      ).r

    lazy val kafkaConsumer = new KafkaConsumer[String, String](readConsumerPropertiesFromConfig)
    kafkaConsumer.subscribe(topicRegex.pattern, new ConsumerRebalanceListener {
      def onPartitionsRevoked(partitions: java.util.Collection[TopicPartition]) = {}
      def onPartitionsAssigned(partitions: java.util.Collection[TopicPartition]) = {}
    })

    def readConsumerPropertiesFromConfig = {
      val properties = new Properties
      properties.put("bootstrap.servers", config.getString(BootstrapServers))
      properties.put("group.id", consumerGroupId)
      properties.put("auto.offset.reset", "earliest")
      properties.put("auto.commit.enable", "false")
      properties.put("key.deserializer", classOf[StringDeserializer].getName)
      properties.put("value.deserializer", classOf[StringDeserializer].getName)

      properties
    }

    /**
      * Process a batch of records with given processor function and commit
      * offsets if it succeeds. Records with null payloads are ignored.
      *
      * @param processor processor function that takes a map of records for different tenants
      * @param batchSize the maximum number of records to process
      */
    def processBatchThenCommit(
      processor: Map[String, Seq[KafkaMember]] ⇒ Try[Map[String, Seq[KafkaMember]]],
      batchSize: Int = 1
    ): Try[Map[String, Seq[KafkaMember]]] =
      doProcess[KafkaMember] { record ⇒
        Option(record.value).map(Json.parse(_).as[KafkaMember])
      }(processor, batchSize)

    /**
      * Process a batch of records with given processor function and commit
      * offsets if it succeeds.
      *
      * Each record is a tuple of the key and the payload deserialised to
      * `Option[T]` which is `None` when the record has a null payload.
      *
      * @param processor processor function that takes a map of records for different tenants
      * @param batchSize the maximum number of records to process
      */
    def processBatchWithKeysThenCommit(
      processor: Map[String, Seq[(String, Option[KafkaMember])]] ⇒ Try[Map[String, Seq[(String, Option[KafkaMember])]]],
      batchSize: Int = 1
    ): Try[Map[String, Seq[(String, Option[KafkaMember])]]] =
      doProcess[(String,  Option[KafkaMember])] { record ⇒
        Some(
          record.key → Option(record.value).map(Json.parse(_).as[KafkaMember])
        )
      }(processor, batchSize)

    def doProcess[T](
      converter: ConsumerRecord[String, String] ⇒ Option[T]
    )(
      processor: Map[String, Seq[T]] ⇒ Try[Map[String, Seq[T]]],
      batchSize: Int = 1
    ): Try[Map[String, Seq[T]]] = {
      val batch = Try {
        import scala.collection.JavaConverters._

        kafkaConsumer.poll(pollMillis).toSeq.flatMap { r ⇒
          val topic = r.topic
          val value = r.value
          val topicRegex(tenant) = r.topic
          converter(r).map(t ⇒ (tenant, t))
        }.groupBy(_._1).mapValues(_.map(_._2))
      }

      for {
        records          ← batch
        processedRecords ← processor(records)
      } yield {
        kafkaConsumer.commitSync()
        processedRecords
      }
    }
  }
}
