/**
 * Generated by apidoc - http://www.apidoc.me
 * Service version: 1.0.0
 */

package test.apidoc.apidoctest.v0.kafka

import scala.util.{ Try, Success }

import org.joda.time.LocalDateTime
import org.mockito.Matchers.any
import org.mockito.Matchers.{ eq ⇒ is }

import com.typesafe.config.ConfigFactory

import movio.testtools.MovioSpec
import movio.testtools.kafka.{ KafkaTestKit, KafkaTestKitUtils }


class KafkaMemberTests extends MovioSpec with KafkaTestKit {
  import test.apidoc.apidoctest.v0.models._

  val kafkaServer = createKafkaServer()
  kafkaServer.startup()

  override def afterAll() = {
    zkServer.stop()
  }

  describe("KafkaMember Producer and Consumer") {
    it("should timeout with no messages") {
      new Fixture {
        awaitCondition("Message should get processed") {
          def processor(messages: Map[String, Seq[KafkaMember]]): Try[Map[String, Seq[KafkaMember]]] =  Success(messages)
          consumer.processBatchThenCommit(processor) shouldBe Success(Map.empty)
        }
        consumer.close()
      }
    }

    it("should send and receive a message") {
      new Fixture {
        // Produce test message
        producer.sendWrapped(entity1, tenant).get
        producer.close()

        // And consume it
        awaitCondition("Message should get processed") {
          def processor(messages: Map[String, Seq[KafkaMember]]): Try[Map[String, Seq[KafkaMember]]] = {
            println("do some side effecting stuff here")
            Success(messages)
          }
          consumer.processBatchThenCommit(processor).get(tenant) shouldBe Seq(entity1)
        }
        consumer.close()
      }
    }

    it("should send and receive a batch of messages") {
      new Fixture {
        val entities = Seq(entity1, entity2)

        // Produce test message
        producer.sendWrapped(entities, tenant).get
        producer.close()

        // And consume it
        awaitCondition("Message should get processed") {
          def processor(messages: Map[String, Seq[KafkaMember]]): Try[Map[String, Seq[KafkaMember]]] =  {
            println("do some side effecting stuff here")
            Success(messages)
          }
          // Use distinct because there are items in the queue from other tests
          consumer.processBatchThenCommit(processor, 100).get(tenant) shouldBe entities
        }
        consumer.close()
      }
    }

    it("messages keys should be available to the processor") {
      new Fixture {
        val entities = Seq(entity1, entity2)

        // Produce test message
        producer.sendWrapped(entities, tenant).get
        producer.close()

        // And consume it
        awaitCondition("Message should get processed") {
          def processor(messages: Map[String, Seq[(String, Option[KafkaMember])]]) = {
            println("do some side effecting stuff here")
            Success(messages)
          }

          // Use distinct because there are items in the queue from other tests
          consumer.processBatchWithKeysThenCommit(processor, 100).get(tenant) shouldBe Seq(
            key1 → Some(entity1),
            key2 → Some(entity2)
          )
        }
        consumer.close()
      }
    }

    it("consumer ignores null payload messages, to support deletes on topics with compaction") {
      new Fixture {
        val topic = KafkaItemTopic.topic(topicInstance)(tenant)
        val rawProducer = createProducer(kafkaServer)

        producer.sendWrapped(entity1, tenant).get
        // Produce null payload message. Need to use the raw producer because the generated producer would
        // throw an exception when trying to convert a null entity to JSON.
        import org.apache.kafka.clients.producer.ProducerRecord
        rawProducer.send(new ProducerRecord[String, String](topic, "anId", null)).get

        producer.sendWrapped(entity2, tenant).get
        producer.close()

        // And consume them
        var consumedEntities = Seq.empty[KafkaMember]
        awaitCondition("All messages should get processed") {
          def processor(messages: Map[String, Seq[KafkaMember]]): Try[Map[String, Seq[KafkaMember]]] =  {
            println("do some side effecting stuff here")
            Success(messages)
          }

          // Use distinct because there are items in the queue from other tests
          consumer.processBatchThenCommit(processor, 100).get.get(tenant).foreach { messages =>
            consumedEntities ++= messages
          }

          consumedEntities shouldBe Seq(entity1, entity2)
        }
        consumer.close()
      }
    }
  }

  trait Fixture {

    val bootstrapServers = kafkaServer.config.hostName + ":" + kafkaServer.config.port
    val topicInstance = "test"
    val tenant = KafkaTestKitUtils.tempTopic()

    val testConfig = ConfigFactory.parseString(s"""
      |configuration {
      |  log-on-startup = false
      |}
      |
      |test.apidoc.apidoctest.kafka {
      |  producer {
      |    bootstrap.servers: "$bootstrapServers"
      |    topic-instance = "$topicInstance"
      |  }
      |}
      |
      |test.apidoc.apidoctest.kafka {
      |  consumer {
      |    bootstrap.servers: "$brokerConnectionString"
      |    topic.instance = "$topicInstance"
      |    tenants = ["ignore_me", "$tenant"]
      |    poll.timeout = 100
      |  }
      |}
      |""".stripMargin)
      .withFallback(ConfigFactory.load())

    val producer = new KafkaMemberProducer(testConfig)
    val consumer = new KafkaMemberConsumer(testConfig, new java.util.Random().nextInt.toString)
    val entity1 = 
    KafkaMember (
      v0 = 
        Member (
          id = 3,
          email = "email1",
          name = None,
          foo = "foo1"
        )
    )
    val key1 = entity1.generateKey(tenant)

    val entity2 = 
    KafkaMember (
      v0 = 
        Member (
          id = 4,
          email = "email2",
          name = None,
          foo = "foo2"
        )
    )
    val key2 = entity2.generateKey(tenant)
  }

}
