/**
 * Generated by apidoc - http://www.apidoc.me
 * Service version: 1.0.0
 */

package test.apidoc.apidoctest.v0.kafka

import scala.util.{ Try, Success }

import org.joda.time.LocalDateTime
import org.mockito.Matchers.any
import org.mockito.Matchers.{ eq ⇒ is }

import com.typesafe.config.ConfigFactory

import movio.testtools.MovioSpec
import movio.testtools.kafka.{ KafkaTestKit, KafkaTestKitUtils }


class KafkaMemberTests extends MovioSpec with KafkaTestKit {
  import test.apidoc.apidoctest.v0.models._

  val kafkaServer = createKafkaServer()
  kafkaServer.startup()

  override def afterAll() = {
    zkServer.stop()
  }

  describe("KafkaMember Producer and Consumer") {
    it("should timeout with no messages") {
      new Fixture {
        awaitCondition("Message should get processed") {
          def processor(messages: Map[String, Seq[KafkaMember]]): Try[Map[String, Seq[KafkaMember]]] =  Success(messages)
          consumer.processBatchThenCommit(processor) shouldBe Success(Map.empty)
        }
        consumer.close()
      }
    }

    it("should send and receive a message") {
      new Fixture {
        // Produce test message
        producer.sendWrapped(entity1, tenant).get
        producer.close()

        // And consume it
        awaitCondition("Message should get processed") {
          def processor(messages: Map[String, Seq[KafkaMember]]): Try[Map[String, Seq[KafkaMember]]] = {
            println("do some side effecting stuff here")
            Success(messages)
          }
          consumer.processBatchThenCommit(processor).get(tenant) shouldBe Seq(entity1)
        }
        consumer.close()
      }
    }

    it("should send and receive a batch of messages") {
      new Fixture {
        val entities = Seq(entity1, entity2)

        // Produce test message
        producer.sendWrapped(entities, tenant).get
        producer.close()

        // And consume it
        awaitCondition("Message should get processed") {
          def processor(messages: Map[String, Seq[KafkaMember]]): Try[Map[String, Seq[KafkaMember]]] =  {
            println("do some side effecting stuff here")
            Success(messages)
          }
          // Use distinct because there are items in the queue from other tests
          consumer.processBatchThenCommit(processor, 100).get(tenant) shouldBe entities
        }
        consumer.close()
      }
    }

    it("messages keys should be available to the processor") {
      new Fixture {
        val entities = Seq(entity1, entity2)

        // Produce test message
        producer.sendWrapped(entities, tenant).get
        producer.close()

        // And consume it
        awaitCondition("Message should get processed") {
          def processor(messages: Map[String, Seq[(String, Option[KafkaMember])]]) = {
            println("do some side effecting stuff here")
            Success(messages)
          }

          // Use distinct because there are items in the queue from other tests
          consumer.processBatchWithKeysThenCommit(processor, 100).get(tenant) shouldBe Seq(
            key1 → Some(entity1),
            key2 → Some(entity2)
          )
        }
        consumer.close()
      }
    }

    it("consumer ignores null payload messages, to support deletes on topics with compaction") {
      new Fixture {
        val topic = KafkaMemberTopic.topic(topicInstance)(tenant)
        val rawProducer = createProducer(kafkaServer)

        producer.sendWrapped(entity1, tenant).get
        // Produce null payload message. Need to use the raw producer because the generated producer would
        // throw an exception when trying to convert a null entity to JSON.
        import org.apache.kafka.clients.producer.ProducerRecord
        rawProducer.send(new ProducerRecord[String, String](topic, "anId", null)).get

        producer.sendWrapped(entity2, tenant).get
        producer.close()

        // And consume them
        var consumedEntities = Seq.empty[KafkaMember]
        awaitCondition("All messages should get processed") {
          def processor(messages: Map[String, Seq[KafkaMember]]): Try[Map[String, Seq[KafkaMember]]] =  {
            println("do some side effecting stuff here")
            Success(messages)
          }

          // Use distinct because there are items in the queue from other tests
          consumer.processBatchThenCommit(processor, 100).get.get(tenant).foreach { messages =>
            consumedEntities ++= messages
          }

          consumedEntities shouldBe Seq(entity1, entity2)
        }
        consumer.close()
      }
    }

    it("should not commit offset if it fails to process a message") {
      val pollTimeout = 200
      // Setup offset topic consumer
      val offsetConsumer = createConsumer(kafkaServer, Map("enable.auto.commit" → "false"))
      offsetConsumer.subscribe(Seq("__consumer_offsets"))

      @tailrec
      def countRemainingOffsets(initialCount: Int = 0): Int = {
        Try {
          offsetConsumer.poll(pollTimeout)
        } match {
          case Success(msgs) if (msgs.count > 0) ⇒ countRemainingOffsets(initialCount + msgs.count)
          case Success(_)                        ⇒ initialCount
          case Failure(e)                        ⇒ throw e
        }
      }

      new Fixture {
        // Init the topic/offsets
        producer.sendWrapped(entity1, tenant).get
        consumer.processBatchThenCommit(Success(_))
        // Wait for auto commit
        Thread.sleep(1000)

        // Consume all the offset records before testing
        countRemainingOffsets()

        // Produce test message
        producer.sendWrapped(entity2, tenant).get
        producer.close()

        def processor(messages: Map[String, Seq[KafkaMember]]): Try[Map[String, Seq[KafkaMember]]] = {
          println("failure on purpose")
          Failure(TestException)
        }

        consumer.processBatchThenCommit(processor) should be a 'failure

        // Wait for auto commit
        Thread.sleep(1000)

        // No offset should be committed
        countRemainingOffsets() shouldBe 0

        consumer.close()
        offsetConsumer.close()
      }
    }

  }

  trait Fixture {

    val bootstrapServers = kafkaServer.config.hostName + ":" + kafkaServer.config.port
    val topicInstance = "test"
    val tenant = KafkaTestKitUtils.tempTopic()

    val testConfig = ConfigFactory.parseString(s"""
      |configuration {
      |  log-on-startup = false
      |}
      |
      |test.apidoc.apidoctest.kafka {
      |  producer {
      |    bootstrap.servers: "$bootstrapServers"
      |    topic.instance = "$topicInstance"
      |  }
      |}
      |
      |test.apidoc.apidoctest.kafka {
      |  consumer {
      |    bootstrap.servers: "$bootstrapServers"
      |    topic.instance = "$topicInstance"
      |    tenants = ["ignore_me", "$tenant"]
      |    poll.timeout = 100
      |  }
      |}
      |""".stripMargin)
      .withFallback(ConfigFactory.load())

    val producer = new KafkaMemberProducer(testConfig)
    val consumer = new KafkaMemberConsumer(testConfig, new java.util.Random().nextInt.toString) {
      override def readConsumerPropertiesFromConfig = {
        val props = super.readConsumerPropertiesFromConfig
        // Configure auto commit interval for testing
        props.put("auto.commit.interval.ms", "500")
        props
      }
    }

    val entity1 = 
    KafkaMember (
      v0 = 
        Member (
          id = 3,
          email = "email1",
          name = None,
          foo = "foo1"
        )
    )
    val key1 = entity1.generateKey(tenant)

    val entity2 = 
    KafkaMember (
      v0 = 
        Member (
          id = 4,
          email = "email2",
          name = None,
          foo = "foo2"
        )
    )
    val key2 = entity2.generateKey(tenant)
  }

}