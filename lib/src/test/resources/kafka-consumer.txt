/**
 * Generated by apidoc - http://www.apidoc.me
 * Service version: 1.0.0
 */

import java.util.Properties

import scala.language.postfixOps
import scala.annotation.tailrec
import scala.collection.JavaConversions._
import scala.util.matching.Regex

import com.typesafe.config.Config

import kafka.consumer._
import kafka.message.MessageAndMetadata
import kafka.serializer.StringDecoder

import play.api.libs.json.Json

import movio.api.kafka_0_8.KafkaConsumer

package test.apidoc.apidoctest.v0.kafka {
  import test.apidoc.apidoctest.v0.models._
  import test.apidoc.apidoctest.v0.models.json._

  object KafkaMemberTopic {
    /**
      The version of the api - apidoc generator enforces this value.
      For use when creating a topic name.
      Example: "v2"
      */
    val apiVersion = "v0"

    /**
      The name of the kafka topic to publish and consume messages from.
      This is a scala statedment/code that that gets executed
      Example: `s"mc-servicename-${apiVersion}-${instance}-${tenant}"`

      @param instance an instance of the topic, eg uat, prod. It's read from the config.
      @param tenant is the customer id, eg vc_regalus
      */
    def topic(instance: String)(tenant: String) = s"mc.data.member-internal.$instance.$tenant"

    /**
      The regex for the kafka consumer to match topics.

      @param instance an instance of the topic, eg uat, prod. It's read from the config.
      @param tenants the tenants of the topics from which the consumer consumes. If it's empty,
             all tenants are matched.
      */
    def topicRegex(inst: String, tenants: Seq[String]) = {
      val instance = Regex.quote(inst)
      val tenantsPattern = if (tenants.isEmpty) ".*"
                           else tenants.map(Regex.quote(_)).mkString("|")

      s"mc.data.member-internal.$instance.($tenantsPattern)"
    }
  }

  object KafkaMemberConsumer {
    val base = "test.apidoc.apidoctest.kafka.consumer"
    val KafkaOffsetStorageType = s"$base.offset-storage-type"
    val KafkaOffsetStorageDualCommit = s"$base.offset-storage-dual-commit"
    val ConsumerTimeoutKey = s"$base.timeout.ms"
    val ConsumerZookeeperConnectionKey = s"$base.zookeeper.connection"
    val TopicInstanceKey = s"$base.topic-instance"
    val TenantsKey = s"$base.tenants"
  }

  /**
    If you choose to override `topicRegex`, make sure the first group captures
    the tenant names.
   */
  class KafkaMemberConsumer (
    config: Config,
    consumerGroupId: String,
    tenants: Option[Seq[String]] = None
  ) extends KafkaConsumer[KafkaMember] {
    import KafkaMemberConsumer._

    lazy val topicRegex: Regex =
      KafkaMemberTopic.topicRegex(
        config.getString(TopicInstanceKey),
        tenants.getOrElse(config.getStringList(TenantsKey))
      ).r

    lazy val topicFilter = new Whitelist(topicRegex.toString)

    lazy val consumerConfig = new ConsumerConfig(readConsumerPropertiesFromConfig)
    lazy val consumer = Consumer.create(consumerConfig)

    lazy val stream: KafkaStream[String, String] =
      consumer.createMessageStreamsByFilter(topicFilter, 1, new StringDecoder, new StringDecoder).head

    lazy val iterator = stream.iterator()

    def readConsumerPropertiesFromConfig = {
      val properties = new Properties

      properties.put("group.id", consumerGroupId)
      properties.put("zookeeper.connect", config.getString(ConsumerZookeeperConnectionKey))
      properties.put("auto.offset.reset", "smallest")
      properties.put("consumer.timeout.ms", config.getString(ConsumerTimeoutKey))
      properties.put("consumer.timeout", config.getString(ConsumerTimeoutKey))
      properties.put("auto.commit.enable", "false")

      properties.put("offsets.storage", config.getString(KafkaOffsetStorageType))
      properties.put("dual.commit.enabled", config.getString(KafkaOffsetStorageDualCommit))

      properties
    }

    def processBatchThenCommit(
      processor: Map[String, Seq[KafkaMember]] ⇒ scala.util.Try[Map[String, Seq[KafkaMember]]],
      batchSize: Int = 1
    ): scala.util.Try[Map[String, Seq[KafkaMember]]] = {
      @tailrec
      def fetchBatch(remainingInBatch: Int, messages: Map[String, Seq[KafkaMember]]): scala.util.Try[Map[String, Seq[KafkaMember]]] ={
        if (remainingInBatch == 0) {
          scala.util.Success(messages)
        } else {
          // FIXME test
          scala.util.Try {
            iterator.next()
          } match {
            case scala.util.Success(message) =>
              val payload = message.message
              val newMessages =
                if (payload != null) {
                  val entity = Json.parse(payload).as[KafkaMember]
                  val topicRegex(tenant) = message.topic

                  val newSeq = messages.get(tenant).getOrElse(Seq.empty) :+ entity

                  messages + (tenant -> newSeq)
                } else {
                  messages
                }

              fetchBatch(remainingInBatch - 1, newMessages)
            case scala.util.Failure(ex) => ex match {
              case ex: ConsumerTimeoutException ⇒
                // Consumer timed out waiting for a message. Ending batch.
                scala.util.Success(messages)
              case ex =>
                scala.util.Failure(ex)
            }
          }
        }
      }

      fetchBatch(batchSize, Map.empty) match {
        case scala.util.Success(messages) =>
          processor(messages) map { allMessages =>
            consumer.commitOffsets(true)
            allMessages
          }
        case scala.util.Failure(ex) => scala.util.Failure(ex)
      }
    }

    def shutdown() = consumer.shutdown()
  }
}
